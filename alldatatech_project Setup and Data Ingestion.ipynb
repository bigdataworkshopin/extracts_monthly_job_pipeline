{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71aa1807-dc2c-435d-85fe-ad67c82f0726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83D\uDE80 Databricks Project Setup & Pipeline Guide: alldatatech_project\n",
    "\n",
    "This document provides step-by-step instructions for setting up the environment, ingesting initial data, and configuring the monthly extraction pipeline within **Databricks** using **Unity Catalog**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ⚙️ Initial Environment Setup & Data Ingestion\n",
    "\n",
    "The steps below are executed using the **`alldatatech_project Setup and Data Ingestion.ipynb`** notebook.\n",
    "\n",
    "### 1.1 Pre-Requisite Actions\n",
    "\n",
    "1.  **Download Files:** Download all necessary project files from the **GitHub** repository to your local machine.\n",
    "2.  **Import Notebook:** Import the notebook named **`alldatatech_project Setup and Data Ingestion.ipynb`** into your Databricks workspace.\n",
    "\n",
    "### 1.2 Notebook Execution Steps\n",
    "\n",
    "Execute the query cells in the imported setup notebook **sequentially** to establish the necessary database objects and prepare for data ingestion.\n",
    "\n",
    "| Step # | Action | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **1** | Creating a New Spark **Catalog** | Defines the top-level namespace (e.g., `alldatatech_project`) in Unity Catalog. |\n",
    "| **2** | Creating a New Spark **Schema** (Database) | Creates a schema within the catalog (e.g., `sales_reporting`) to organize tables. |\n",
    "| **3** | Creating the **`enrollment_status_log`** Delta Table | Defines the target Delta Lake table for the ingested data. |\n",
    "| **4** | Creating a Unity Catalog **Volume** | Sets up a Unity Catalog **Volume** to manage access to cloud storage for raw files (e.g., CSVs). |\n",
    "| **5** | **Upload Sample Data** (Manual) | **MANUAL ACTION:** Upload the file **`sample_enrollment_data.csv`** to the following Volume path: **`alldatatech_project.sales_reporting.enrollment_files`**. |\n",
    "| **6** | PySpark: Ingesting CSV Data | Reads the uploaded CSV file from the Volume and loads it into the **`enrollment_status_log`** Delta table. |\n",
    "| **7** | Viewing All Data | Validates successful ingestion by displaying the contents of the target table. |\n",
    "| **8** | Creating a Subdirectory | Creates a new subdirectory within the Volume path (e.g., for archiving processed files). |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. \uD83D\uDE80 Configuring the Extraction Pipeline\n",
    "\n",
    "This section configures the monthly extraction process using a Databricks Job pipeline.\n",
    "\n",
    "### 2.1 Job Creation via YAML\n",
    "\n",
    "1.  **Navigate to Jobs:** Go to **Workflows** (or Jobs & Pipelines) in the Databricks workspace sidebar.\n",
    "2.  **Create Job:** Click **\"Create Job\"**.\n",
    "3.  **Edit as YAML:** Switch the job creation interface to the **\"Edit as YAML\"** view.\n",
    "4.  **Paste YAML Code:** Copy the entire content of the **`wf_extracts_monthly.yml`** file and paste it into the YAML editor.\n",
    "5.  **Save:** Save the job configuration. This creates the pipeline named **`wf_extracts_monthly`**.\n",
    "\n",
    "### 2.2 Notebook Preparation\n",
    "\n",
    "1.  **Import Extraction Notebook:** Import the notebook named **`Extract.ipynb`** into your Databricks workspace.\n",
    "2.  **Update Pipeline Path:**\n",
    "    * Open the newly created job **`wf_extracts_monthly`**.\n",
    "    * Inspect the notebook task within the job.\n",
    "    * **Update the Notebook Path** in the job task definition to point to the exact location of the imported **`Extract.ipynb`** file in your workspace.\n",
    "\n",
    "### 2.3 Running the Pipeline\n",
    "\n",
    "1.  **Execute Job:** Open the **`wf_extracts_monthly`** job.\n",
    "2.  Click **\"Run now\"** to execute the pipeline and initiate the data extraction process.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Artifacts Summary\n",
    "\n",
    "| Artifact | Type | Purpose |\n",
    "| :--- | :--- | :--- |\n",
    "| **`alldatatech_project Setup and Data Ingestion.ipynb`** | Notebook | One-time setup of UC Catalog, Schema, Volume, and Delta Table. |\n",
    "| **`sample_enrollment_data.csv`** | Raw Data File | Initial data source uploaded to the Volume. |\n",
    "| **`wf_extracts_monthly.yml`** | YAML Definition | Configuration file for the Databricks Job (Pipeline). |\n",
    "| **`Extract.ipynb`** | Notebook | Contains the primary ETL/extraction logic run by the job. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a3fa5e0-4143-4dbd-b997-270bcc304a50",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating a New Spark Catalog"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE CATALOG alldatatech_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe1f612-efca-4cd0-ac37-7368046bdb61",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating a New Spark Schema (Database)"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA alldatatech_project.sales_reporting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf81237-4dfd-49ce-a9b7-0aa28c86b1e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating the enrollment_status_log Delta Table"
    }
   },
   "outputs": [],
   "source": [
    " %sql\n",
    "CREATE TABLE alldatatech_project.sales_reporting.enrollment_status_log (\n",
    "  fes_enrollment_id STRING COMMENT 'The Unique enrollment Id',\n",
    "  fes_revision_identifier STRING COMMENT 'Revision Identifier changes every time there is an update to the record',\n",
    "  enrollment_update_ts TIMESTAMP COMMENT 'The date this record was last modified',\n",
    "  dealer_name STRING COMMENT 'The dealership name',\n",
    "  pacode STRING COMMENT 'The Part and Accessory Code that uniquely identifies a dealer',\n",
    "  country STRING COMMENT 'The country determines if the record is from US or Canada',\n",
    "  dealer_status STRING COMMENT 'The dealer status : Active, Terminated, Enrolled ,etc',\n",
    "  product_name STRING COMMENT 'The Name of the Product the dealer is enrolled into',\n",
    "  sku_name STRING COMMENT 'The name of the SKU. Every product is sub categorized based on SKU',\n",
    "  product_key STRING COMMENT 'The key or code of the Product the dealer is enrolled into',\n",
    "  sku_key STRING COMMENT 'The key or code of the SKU. Every product is sub categorized based on SKU',\n",
    "  product_and_sku_key STRING COMMENT 'This combination is unique for a given product and Sku',\n",
    "  sku_type STRING COMMENT 'The type of product, whether its an Add-on, Tier, etc',\n",
    "  brand_type STRING COMMENT 'The brand type of the dealer: Dual/Rooftop , Ford, Lincoln, Quicklane',\n",
    "  brand_name STRING COMMENT 'The Brand Name of the dealer.Can be Ford, Lincoln or QuickLane',\n",
    "  enrollment_type STRING COMMENT 'The Enrollment Type',\n",
    "  enrollment_status STRING COMMENT 'The Enrollment Status : Enrolled , Terminated',\n",
    "  enrollment_secondary_status STRING COMMENT 'The Secondary status of the enrollment, Active, Inactive',\n",
    "  quantity INT COMMENT 'The number of enrollments bought by the dealer, Mostly 1',\n",
    "  one_time_fee DECIMAL(10,0) COMMENT 'The one-time fee for the product enrollment',\n",
    "  monthly_fee DECIMAL(10,0) COMMENT 'The montly fee for the product enrollment. Most of the products have monthly fees',\n",
    "  annual_fee DECIMAL(10,0) COMMENT 'The annual fee of the product enrollment',\n",
    "  fields STRING COMMENT 'The additional details with respect to product enrollment',\n",
    "  enrollment_ts TIMESTAMP COMMENT 'The enrollment date of the dealer for the product',\n",
    "  activation_ts TIMESTAMP COMMENT 'The activation date of the product',\n",
    "  termination_ts TIMESTAMP COMMENT 'The date the dealer terminated form the product',\n",
    "  termination_reason STRING COMMENT 'The reason why the dealer terminated from the productd',\n",
    "  primary_contact_name STRING COMMENT 'The primary contact name for the product',\n",
    "  primary_contact_title STRING COMMENT 'The Title of the primary contact name for the product',\n",
    "  primary_contact_phone STRING COMMENT 'The phone number of the primary contact for the product', -- Added missing COMMENT for phone\n",
    "  primary_contact_email STRING COMMENT 'The email of the primary contact for the product',\n",
    "  create_user STRING COMMENT 'The user that created this record',\n",
    "  create_epoch BIGINT COMMENT 'The create date epoch',\n",
    "  create_ts STRING COMMENT 'The create date timestamp',\n",
    "  update_user STRING COMMENT 'The user that updated this record',\n",
    "  update_epoch BIGINT COMMENT 'The update date epoch ',\n",
    "  update_ts STRING COMMENT 'The update date timestamp',\n",
    "  file_date STRING COMMENT 'The date of the file or the record'\n",
    ")\n",
    "USING delta\n",
    "COMMENT 'Log of dealer enrollment statuses for sales reporting.' -- Added a descriptive table comment\n",
    "TBLPROPERTIES (\n",
    "  'delta.minReaderVersion' = '1',\n",
    "  'delta.minWriterVersion' = '2',\n",
    "  'external.table.purge' = 'true'\n",
    "  -- Removed extraneous or legacy TBLPROPERTIES like 'STATS_GENERATED' and 'bucketing_version' for a clean new UC table\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38089da0-9c95-4de7-afce-118a3963ca06",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating a Unity Catalog Volume"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE VOLUME alldatatech_project.sales_reporting.enrollment_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "897b53b1-f211-445e-a1e5-8f35fd5e52be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MANUAL ACTION: Upload the file sample_enrollment_data.csv to the following Volume path: alldatatech_project.sales_reporting.enrollment_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6867d23c-dd65-447a-b931-5c3b9ffa20e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark: Ingesting CSV Data from Volume into Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV into DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"nullValue\", \"null\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(\"/Volumes/alldatatech_project/sales_reporting/enrollment_files/sample_enrollment_data.csv\") \n",
    "display(df)\n",
    "# print(\"Columns in DataFrame:\", df.columns)\n",
    "\n",
    "# Insert data into existing table\n",
    "df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .insertInto(\"alldatatech_project.sales_reporting.enrollment_status_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb73682-8d1e-4379-b699-a516f4b67ae8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Viewing All Data inenrollment_status_log Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from alldatatech_project.sales_reporting.enrollment_status_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce35dca-0b84-4130-8621-14ee9b1c1af4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating a Subdirectory within the Volume"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/alldatatech_project/sales_reporting/enrollment_files/reporting\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 9003534405064013,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "alldatatech_project Setup and Data Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}